[
  {
    "objectID": "License.html",
    "href": "License.html",
    "title": "Cornelius Tanui",
    "section": "",
    "text": "corneliustanui.rbind.io © by Cornelius Tanui, 2024, licensed under CC BY-NC 4.0.\nrbind.io © by The Rbind Team, licensed under MIT License."
  },
  {
    "objectID": "License.html#website-and-content",
    "href": "License.html#website-and-content",
    "title": "Cornelius Tanui",
    "section": "",
    "text": "corneliustanui.rbind.io © by Cornelius Tanui, 2024, licensed under CC BY-NC 4.0.\nrbind.io © by The Rbind Team, licensed under MIT License."
  },
  {
    "objectID": "License.html#disclaimer",
    "href": "License.html#disclaimer",
    "title": "Cornelius Tanui",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views expressed here and on my website are my own and do not reflect the position of my employer or any organisation I am associated with."
  },
  {
    "objectID": "License.html#code-of-conduct",
    "href": "License.html#code-of-conduct",
    "title": "Cornelius Tanui",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThe content on this website is intended for educational and training purposes only, and is NOT by any means a platform to cause and perpetrate harm, destruction, prejudice, conflict, and any other outcome that can reasonably be considered harmful.\nBy visiting this website, you agree to abide by this moral, just, friendly, and charitable obligation.\nIn the event that whole or part of the content is considered inappropriate and/or offensive, contact the author for revision and update, or outright removal of the entire material."
  },
  {
    "objectID": "content/projects.html",
    "href": "content/projects.html",
    "title": "Projects",
    "section": "",
    "text": "1. Quick Stats Reporter(QSR)\nThis R Shiny web application is an extremely useful tool for quick data analysis and reporting. It determines on it’s own what kind of table and graph to generate depending on the type of variable(s) selected.\n\n\n2. 2019 KPHC Analytics\nAn R Shiny web dashboard application to demonstrate various findings of the 2019 Kenya Population and Housing Census. The app is auto-deployed using GitHub workflows and Docker CI/CD.\n\n\n3. miscellaneousR\nThis is an R package hosted on GitHub. The package contains convenient functions for performing simple to complex tasks appertaining to data wrangling."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "",
    "text": "Image source: Imagine Art"
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#sec-shallow",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#sec-shallow",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "1.1 My Shallow Thoughts on MTI",
    "text": "1.1 My Shallow Thoughts on MTI\nWhen I first heard of MTI, my immediate thought was that the government of Kenya had finally embraced Artificial Intelligence on a larger scale and decided to award university students scholarships based on economic bands decided by some novel AI algorithm. Think, a combination of classification algorithms of ‘high compute, high repute’. A pleasant thought, right? No.\nNo because, later on, I searched for MTI online and found out that it stands for ‘means testing instrument’, and if you are deep into data, you would think ‘means’ is hereby used to denote average. See, ‘testing of means’ is not remotely uncommon, we come across it all the time in data analytics. T-test is a test of means. However, ‘means’ in the context of MTI stands for resources, or assets’, that a student has access to that could be used to fund their higher education. ‘Means’ can be a confusing word. ‘Means of transport’, ‘by all means’, etc.\nAs it turned out, MTI is a widely used concept in the education and social protection sectors, and I was embarrassingly waaay off in thinking that it had something to do with statistical averages.\nI was waaay off in yet, yet again, in a different aspect – the students joining university in September, 2024 as first-years/freshmen had never been banded before, and therefore, there would be no training data for my imagined AI model! This is the first time banding is happening in Kenya, as regards university funding, so maybe, there will be (enough) data to train a model in 2025, so that the freshmen of 2025 will have successfully been banded by AI."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#background-to-mti",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#background-to-mti",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "1.2 Background to MTI",
    "text": "1.2 Background to MTI\nGlobally, MTI has been around for a while now, with the first documented use in 1930s involving provision of relief to households by governments. If a home was deemed able to support itself by the source of income it had, the the government benefits were stopped or reduced1. MTI has since been heavily employed in the social protection to provide targeted anti-poverty benefits to households, civil legal aid to individuals2, communities, and geographies. The obvious reason for preference of MTI to universal provision of support – such as universal basic income – is that MTI offers the support to targeted beneficiaries, because with the universal approach, there may be recipients who do not genuinely require it3.\nIn Kenya, MTI has been used for a long time to identify households in marginalized communities that are eligible for benefit from cash transfers4 under the National Safety Net Programmes (NSNP). One such safety programme is the Hunger Safety Net Programme (HSNP) that supports old persons, orphans and vulnerable children, and persons with severe disability.\nLiterature indicates that MTI has worked successfully so far in Kenya as implemented under NSNP, yet it is not without shortcomings. For example, the popular controversy around it is, it discourages the target population from engaging in financial savings5, consequently promoting poverty, a concept known as poverty trap6. MTI sustained an unmitigated uproar over it’s banding inaccuracy7 that led to placement of students from poor backgrounds into higher bands that require them to dig deep into their pockets to fill the gap, pockets which they either do not have, or are torn. The bands range from 1 (least able) to 5 (most able.)\nUnder the hood, MTI is mainly a regression model – such as a tobit model – that aggregates various variables together and provides a value3 which is then compared to a threshold that determines whether the candidate qualifies for the benefit, or does not. Principal components analysis models have also been deployed to this cause4.\nNow, let us explore how a machine learning (ML) classifier could be used as an alternative to MTI to award financial support to university students in Kenya."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#data-simulation",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#data-simulation",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.1 Data Simulation",
    "text": "2.1 Data Simulation\nSimulation of these data is outside the scope of this article and is covered in my other post.\nThe table below shows the properties of the variables are;\n\n\n\nTable 1: Properties of variables\n\n\n\n\n\n\n\n\n\n\nVariable\nData type\nDistribution\n\n\n\n\nBands\nOrdinal\nMultinomial\n\n\nGross family income\nx ∈ ℝ+\nNegative Binomial\n\n\nGeographical location\nNominal\nUniform\n\n\nPoverty probability index\nx ∈ ℝ+\nSkewed Normal\n\n\nSpecial circumstances such as orphans\nBinary\nBinomial\n\n\nStudents with disability\nBinary\nBinomial\n\n\nNumber of dependents\nx ∈ ℕ+\nPoisson\n\n\nProgram costs\nx ∈ ℝ+\nSkewed Normal\n\n\nGender\nNominal\nMultinomial"
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#descrpitive-analysis",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#descrpitive-analysis",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.2 Descrpitive Analysis",
    "text": "2.2 Descrpitive Analysis\n\n\n\n\n\n\nDisclaimer!\n\n\n\n\n\nThe data is simulated, and therefore substantially differs with the actual scenario! The data is meant for learning purposes only, and the statistical estimates reported MUST NOT be taken as true reflection of the real-word situation.\n\n\n\nThe simulated data looks like this;\n\n\nCode\n# table display setup\n#| label: tbl-simulated_data .striped .hover .primary .bordered\n#| tbl-cap: \"Simulated data\"\n#| tbl-cap-location: bottom \n\n# load data\nsimulated_data &lt;- readRDS(here::here(\"./Data/simulated_data.rds\"))\n\n# view data (printed on your browser)\nknitr::kable(head(x = simulated_data, n = 5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBands\nGrossFamilyIncome\nGeographicalLocation\nPovertyProbabilityIndex\nOrphans\nDisability\nNumberOfDependents\nProgramCostsKES\nGender\n\n\n\n\n1\n1154\n9\n1.0563022\n0\n0\n10\n509734.8\n1\n\n\n1\n2400\n5\n0.9868637\n0\n0\n5\n469997.6\n1\n\n\n1\n2411\n27\n0.9790371\n1\n0\n1\n547886.0\n1\n\n\n1\n2494\n22\n0.9679570\n0\n0\n3\n420495.4\n1\n\n\n1\n2559\n41\n0.9642283\n1\n0\n4\n530990.7\n1"
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#inferential-model-creation",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#inferential-model-creation",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.3 Inferential Model Creation",
    "text": "2.3 Inferential Model Creation\nBefore we create a model that predicts or assigns bands, let’s first explore the relationship between the outcome (Bands) and all the predictors.\n\n\nCode\nmti_model_fit &lt;- parsnip::multinom_reg() |&gt; \n  parsnip::fit(Bands ~ ., data = simulated_data)\n\n\nNow, we can extract the meaning from the information contained in the model created above. Instead of reporting log of odds, we can report just odd, as that make sit easier to interpret;-\n\n\nCode\nmodel_results &lt;- broom::tidy(mti_model_fit, exponentiate = TRUE, conf.int = TRUE) |&gt;\n  dplyr::select(Bands = y.level, \n                Predictors = term,\n                Odds = estimate,\n                StandardError = std.error,\n                Statistic = statistic,\n                `P-value` = p.value,\n                Lower95 = conf.low,\n                Upper95 = conf.high\n                ) |&gt; \n  \n  dplyr::mutate(Odds = round(Odds, 4),\n                Lower95 = round(Lower95, 4),\n                Upper95 = round(Upper95, 4))\n\nmodel_results\n\n\n# A tibble: 216 × 8\n   Bands Predictors             Odds StandardError Statistic `P-value`   Lower95\n   &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 2     (Intercept)       Inf            1.02e- 7   8.92e10         0 Inf      \n 2 2     GrossFamilyIncome   8.08e-1      5.51e- 4  -3.88e 2         0   8.07e-1\n 3 2     GeographicalLoca…   5.48e+0      1.47e- 8   1.16e 8         0   5.48e+0\n 4 2     GeographicalLoca…   5.75e+3      1.13e- 9   7.66e 9         0   5.75e+3\n 5 2     GeographicalLoca…   4   e-4      3.44e- 8  -2.27e 8         0   4   e-4\n 6 2     GeographicalLoca…   2.73e+2      1.36e- 9   4.14e 9         0   2.73e+2\n 7 2     GeographicalLoca…   1.37e+2      5.31e- 9   9.26e 8         0   1.37e+2\n 8 2     GeographicalLoca…   8.7 e-3      2.86e- 8  -1.66e 8         0   8.7 e-3\n 9 2     GeographicalLoca…   0            3.19e- 8  -4.16e 8         0   0      \n10 2     GeographicalLoca…   1.66e+6      1.40e-16   1.02e17         0   1.66e+6\n# ℹ 206 more rows\n# ℹ 1 more variable: Upper95 &lt;dbl&gt;\n\n\nA multinomial regression is, under the hood, an ensemble of several “binary logistic regressions”. As we can see from the results above, the outcome Bands contains all the outcome bands (technically called ‘classes’). Each class creates a binary logistic regression, and that is why we see all the Predictors for each class. Band 1 (class 1) is not reported because it is taken as the reference group. Any class can be made a reference groub by relevelling the factors. For example, to make band 5 the reference class, simulated_data$Bands &lt;- relevel(simulated_data$Bands, ref = \"5\").\nLooking at the p-values at 95% level of confidence, some independent indicators are significant predictors of certain bands. For instance, gross family income is significant at all bands (p-value &lt;0.0001) respectively.) Therefore, a unit increase of a gross family income increases the odds of the family being in band 4 than being in band 1 by 25.07%, holding all other predictors constant. The value of 25.07% is gotten by (1.2507 - 1)*100 = 25.07\\%."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#prediction-model-creation",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#prediction-model-creation",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.4 Prediction Model Creation",
    "text": "2.4 Prediction Model Creation\nFor a start, we shall define the the model pipeline using the conventional tidymodels process workflow with parsnip, and specify glmnet as the engine. Later on, maybe in a separate post, we will explore other engines such as brulee, nnet, and keras. The keras engine is a Python library that depends on tensorflow, another Python library that must be installed. To install keras and tensorflow, open Anaconda Command Prompt and run the commands pip install keras, and pip install tensorflow and then load the R package equivalents of the two python libraries, along with other necessary packages, as follows: -\n\n\nCode\n## load packages\nlibrary(tidyverse)  # data processing packages\nlibrary(tidymodels) # model definition packages\nlibrary(parsnip)    # model manipulation functions\n\nlibrary(glmnet)     # model processing engine\n# library(spark)      # model processing engine\n# library(keras)      # model processing engine (requires package tensorflow)\n# library(tensorflow) # The Python package needs to be installed\n# library(nnet)       # model processing engine\n# library(brulee)     # model processing engine (requires libtorch distro of PyTorch)\n\n\n\n2.4.1 Generate Training and Testing Datasets\n\n\nCode\n## create training and testing sets\n# set seed for reproducibility of the sets\nset.seed(44)\ndata_split &lt;- initial_split(simulated_data, prop = 0.75)\n\n# 75% of records\ntrain_data &lt;- training(data_split) \n\n# 25% of records\ntest_data  &lt;- testing(data_split)\n\n\n\n\n2.4.2 Create Initial Model\nIn this model, there are only two (hyper)parameters that need to be specified: penalty and mixture. The value of the penalty defines the degree to which regularization is applied to the model. Regularization is a technique used to control overfitting by adding a penalty (error) term to the model. On the other hand, mixture is the value that species how the penalty term is added. There are three ways of adding this penalty11;\n\nLasso (also called L1 regularization): The penalty term added to the model is the sum of absolute coefficients of the predictors, multiplied by some constant – λ∑ |β|. If mixture = 1 then the resulting model is pure L1.\nRidge (also called L2 regularization): The penalty term added to the model is the sum of squares of coefficients of the predictors, multiplied by some constant – λ∑ β^2. If mixture = 0 then the resulting model is pure L2.\nElastic net (a mix of L1 and L2): The penalty term added to the model is the sum of L1 and L2 – λ∑ |β| + λ∑ β². If 0 &lt; mixture &lt; 1 then the resulting model is elastic net.\n\n\n\nCode\n# create the null model\nmultinom_reg_glmnet_spec &lt;-\n  # parsnip::multinom_reg(penalty = tune(), mixture = tune()) |&gt; # to be tuned later\n  parsnip::multinom_reg(penalty = double(1), mixture = double(1)) |&gt; # manual starting values\n  set_engine('glmnet') |&gt;\n  set_mode(\"classification\")\n\n\n\n\n2.4.3 Recipe Definition\nCreate pre-processing steps for the predictors (also called features.);\n\n\nCode\n# create the recipe\nmultinom_recipe &lt;- \n  \n  # specify the outcome variable\n  recipe(Bands ~ ., data = train_data) |&gt;\n  \n  # specify predictors to be one-hot-encoded\n  step_dummy(GeographicalLocation, Gender, Orphans, Disability) |&gt;\n  \n  # center all normally distributed predictors  \n  step_center(GrossFamilyIncome, PovertyProbabilityIndex, NumberOfDependents, ProgramCostsKES) |&gt;\n  \n  # scale all normally distributed predictors \n  step_scale(GrossFamilyIncome, PovertyProbabilityIndex, NumberOfDependents, ProgramCostsKES) |&gt;\n  \n  # normalize all numeric variables\n  step_normalize(GrossFamilyIncome, PovertyProbabilityIndex, NumberOfDependents, ProgramCostsKES)\n\n\n\n\n2.4.4 Workflow Definition\nA workflow defines the order in which the model is build by sequentially combining pre-processing steps (recipe) and other pipeline elements into a workflow;\n\n\nCode\n# Create the workflow\nmultinom_workflow &lt;- \n  workflow() |&gt;\n  add_recipe(multinom_recipe) |&gt;\n  add_model(multinom_reg_glmnet_spec)"
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#model-training",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#model-training",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.5 Model Training",
    "text": "2.5 Model Training\nWe can now train(fit) the model using the training data as follows;\n\n\nCode\n# train model\nmultinom_fit &lt;-\n  multinom_workflow |&gt;\n  parsnip::fit(data = train_data)"
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#model-performance-diagnostics",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#model-performance-diagnostics",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.6 Model Performance Diagnostics",
    "text": "2.6 Model Performance Diagnostics\nNow that we have a ‘bare bones’ model, let’s find out it’s performance using metrics from the yardstick package, which is part of tidymodels. There are many ways to calculate multiclass metrics, such as using confusion matrix and accuracy level. Note that ROC (receiver operating characteristic) curve is used for binary classifiers, not multiclass classifiers which is the focus of this article.\n\n\nCode\n# create new data (can be completely new or use training data without resposne variable)\npredictors_data &lt;- test_data |&gt; dplyr::select(-Bands)\n\n# use the model and the training data to get predictions of bands\nbands_data &lt;- test_data\nbands_data$Bands_pred &lt;- predict(multinom_fit, new_data = predictors_data, type = \"class\")\nbands_data$Bands_pred &lt;- bands_data$Bands_pred$.pred_class\n\nbands_data &lt;- bands_data |&gt; dplyr::select(Bands, Bands_pred)\n\n## measure performance\n# 1) confusion matrix\nconf_mat(data = bands_data, truth = Bands, estimate = Bands_pred)\n\n\n          Truth\nPrediction   1   2   3   4   5\n         1 819  83   0   0   0\n         2   3 931 135   0   0\n         3   0   0 382 120   0\n         4   0   0   0   1   2\n         5   0   0   0   2  22\n\n\nCode\n# 2) kappa and accuracy\nmetrics(data = bands_data, truth = Bands, estimate = Bands_pred)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.862\n2 kap      multiclass     0.793\n\n\nCode\n# 3) precision (same as accuracy)\nprecision(data = bands_data, truth = Bands, estimate = Bands_pred, estimator = \"micro\")\n\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision micro          0.862\n\n\nThere you go; a model accuracy of 0.862 is pretty high, given that the data was simulated! So much good news here, almost too good to be true. And what’s more, this is a basic model, i.e. the (hyper)parameters penalty and mixture have the conservative values of doubel(1). Are we able to improve the accuracy further by changing these values, i.e. by tuning them? Let’s find out below."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#hyperparameter-tuning",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#hyperparameter-tuning",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.7 Hyperparameter Tuning",
    "text": "2.7 Hyperparameter Tuning\nNote that the mixture and penalty levels (values) are arbitrarily set to 3 and 2 respectively – these are simply starting points, and can be any numbers but the closer they are to the optimal values the quicker it is for the grid search to converge to those optimal values, which saves the computer tonnes of processing resources and time. The same goes for cross-validation of 3 folds.\n\n\nCode\n# define model\nmultinom_reg_glmnet_spec_tuned &lt;-\n  parsnip::multinom_reg(penalty = tune(), mixture = tune()) |&gt; # to be tuned\n  set_engine('glmnet') |&gt;\n  set_mode(\"classification\")\n\n# define grid search\nmultinom_grid &lt;- grid_regular(mixture(), penalty(), levels = c(mixture = 3, penalty = 2))\n\n# define work flow\nmultinom_wf &lt;- workflow() %&gt;%\n  add_recipe(multinom_recipe) |&gt;\n  add_model(multinom_reg_glmnet_spec_tuned)\n\n# define 3-fold CV resamples from which to search best parameter values\nmultinom_3f_cv_folds &lt;- vfold_cv(data = train_data, v = 3)\n\n# tune the hyperparameters using the grid search\nmultinom_tuned &lt;- tune_grid(\n  multinom_wf,\n  resamples = multinom_3f_cv_folds,\n  grid = multinom_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nselect_best(multinom_tuned, metric = \"accuracy\")\n\n\n# A tibble: 1 × 3\n       penalty mixture .config             \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 0.0000000001       1 Preprocessor1_Model5\n\n\nFrom the above grid search, we got penalty = 0.0000000001 and mixture = 1. Let’s plug these values into the model and see if the accuracy has improved;\n\n\nCode\n# create the null model\nmultinom_reg_glmnet_spec &lt;-\n  parsnip::multinom_reg(penalty = 0.0000000001, mixture = 1) |&gt;\n  set_engine('glmnet') |&gt;\n  set_mode(\"classification\")\n\n# rerun the workflow now that model has been updated\nmultinom_workflow &lt;- \n  workflow() |&gt;\n  add_recipe(multinom_recipe) |&gt;\n  add_model(multinom_reg_glmnet_spec)\n\n# re-train the model based on new parameters\nmultinom_fit &lt;-\n  multinom_workflow |&gt;\n  parsnip::fit(data = train_data)\n\n## measure performance of new model\n# create new data (can be completely new or use training data without resposne variable)\npredictors_data &lt;- test_data |&gt; dplyr::select(-Bands)\n\n# use the model and the training data to get predictions of bands\nbands_data &lt;- test_data\nbands_data$Bands_pred &lt;- predict(multinom_fit, new_data = predictors_data, type = \"class\")\nbands_data$Bands_pred &lt;- bands_data$Bands_pred$.pred_class\n\nbands_data &lt;- bands_data |&gt; dplyr::select(Bands, Bands_pred)\n\n## measure performance\n# 1) confusion matrix\nconf_mat(data = bands_data, truth = Bands, estimate = Bands_pred)\n\n\n          Truth\nPrediction    1    2    3    4    5\n         1  817    2    0    0    0\n         2    5 1006    6    0    0\n         3    0    6  506    2    0\n         4    0    0    5  121    0\n         5    0    0    0    0   24\n\n\nCode\n# 2) kappa and accuracy\nmetrics(data = bands_data, truth = Bands, estimate = Bands_pred)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.990\n2 kap      multiclass     0.985\n\n\nCode\n# 3) precision (same as accuracy)\nprecision(data = bands_data, truth = Bands, estimate = Bands_pred, estimator = \"micro\")\n\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision micro          0.990\n\n\nThere is a huge improvement on model performance; we went from an accuracy of 0.862 to an accuracy of 0.9896! This is no small feat at all. Wonder if this can be improved further? Maybe, maybe not. At this point, there is no sensible need to go further."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#band-prediction",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#band-prediction",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "2.8 Band Prediction",
    "text": "2.8 Band Prediction\nNow that we have a reasonably reliable model, we can ask it to place a student into bands. All we need to do is provide the values of “GrossFamilyIncome”, “GeographicalLocation”, “PovertyProbabilityIndex”, “Orphans”, “Disability”, “NumberOfDependents”, “ProgramCostsKES”, and “Gender” for a particular student in the new_data argument of the predict function, as follows;\n\n\nCode\n# enter new student data\nnew_student_data &lt;- data.frame(\n  GrossFamilyIncome = 3000,\n  GeographicalLocation = factor(5),\n  PovertyProbabilityIndex = 0.7,\n  Orphans = factor(1), \n  Disability = factor(1), \n  NumberOfDependents = 5, \n  ProgramCostsKES = 500000, \n  Gender = factor(1)\n)\n\n# predict new student band\nnew_student_band &lt;- predict(multinom_fit, new_data = new_student_data, type = \"class\")\n\nnew_student_band\n\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 1          \n\n\nThe new student above, as reasonably expected, is assigned band 1. “Reasonably expected’ because the student comes from a family of low gross income, high poverty probability index, and is orphaned and disabled."
  },
  {
    "objectID": "content/posts/MTI_Modeling_2024-09-06/index.html#footnotes",
    "href": "content/posts/MTI_Modeling_2024-09-06/index.html#footnotes",
    "title": "Machine Learning Classifier for Banding University Students in Kenya",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nvan Oorschot, W. J. H., & Schell, J. (1991). Means-testing in Europe: A growing concern. In M. Adler, C. Bell, J. Clasen, & A. Sinfield (Eds.), The sociology of social security (pp. 187-211). (Edinburgh education and society series). Edinburgh University Press.↩︎\nhttps://www.gov.uk/guidance/criminal-legal-aid-means-testing↩︎\nBrown, C., Ravallion, M., & Van de Walle, D. (2016). A poor means test. Econometric targeting in Africa. The World Bank.↩︎\nVilla, Juan M. [2016] A harmonised proxy means test for Kenya’s National Safety Net programme. GDI Working Paper 2016-003. Manchester: The University of Manchester.↩︎\nElizabeth T. Powers,Does means-testing welfare discourage saving? evidence from a change in AFDC policy in the United States, Journal of Public Economics, Volume 68, Issue 1, 1998, Pages 33-53, ISSN 0047-2727, https://doi.org/10.1016/S0047-2727(97)00087-X. (https://www.sciencedirect.com/science/article/pii/S004727279700087X)↩︎\nKraay, Aart, and David McKenzie. 2014. “Do Poverty Traps Exist? Assessing the Evidence.” Journal of Economic Perspectives, 28 (3): 127–48.↩︎\nhttps://www.citizen.digital/news/govt-explains-why-many-students-miss-out-on-scholarships-under-the-new-funding-model-n348207↩︎\nhttps://kafu.ac.ke/images/2022/Academics/nfm/NEW_FUNDING_MODEL_-_6TH_AUGUST_2024.pdf↩︎\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.↩︎\nKook, L., Herzog, L., Hothorn, T., Dürr, O., & Sick, B. (2022). Deep and interpretable regression models for ordinal outcomes. Pattern Recognition, 122, 108263.↩︎\nhttps://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/↩︎"
  },
  {
    "objectID": "content/blog.html",
    "href": "content/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Basics of Statistical Data Simulation\n\n\n\n\n\n\nR\n\n\nSimulation\n\n\nMathematical Statistics\n\n\n\nUsing Statistical Distributions to Generate Data that Mimics the Real World Scenario\n\n\n\n\n\nSep 12, 2024\n\n\nCornelius Tanui\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Classifier for Banding University Students in Kenya\n\n\n\n\n\n\nR\n\n\nClassification\n\n\nPrediction\n\n\nMTI\n\n\n\nPredicting the Household Economic Bands Into Which University Students Fall for Award of Financial Support from the Kenyan Government.\n\n\n\n\n\nAug 6, 2024\n\n\nCornelius Tanui\n\n\n\n\n\n\n\n\n\n\n\n\nModelling CPI Using ARIMA(p,d,q)\n\n\n\n\n\n\nR\n\n\ntime-series\n\n\nSmoothing\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nCornelius Tanui\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/posts/ARIMA_Fitting_2024-08-22/index.html",
    "href": "content/posts/ARIMA_Fitting_2024-08-22/index.html",
    "title": "Modelling CPI Using ARIMA(p,d,q)",
    "section": "",
    "text": "Image source: Imagine Art\n\n\n\n1) Why CPI is Time Series\nConsumer Price Index (CPI) is defined as a measure of the weighted aggregate change in retail prices paid by consumers for a given basket of goods and services.\nThe CPI is a statistical indicator of changes in consumer prices experienced by citizens of a country. It is a measure that compares the cost of a fixed basket (234 items) of goods and services purchased by consumers over time. The CPI index reflects only pure price change and is a widely used to monitor the rate of inflation, which is the percentage change of CPI over one year (The Kenya National Bureau of Statistics (KNBS), 2010).\nCPI is a univariate non-stationary time series variable which does not have significant seasonality. Univariate because it is a single variable that predicts (or correlates to) itself based on its own history (auto-regressive) as opposed to an ordinary regression variable that is predicted by one or more variables of different nature. Furthermore, CPI is a time series variable since observed values are indexed in time. The fact that it naturally has an upward trend, possibly due to increasing population, changing lifestyles of people, and a generally growing economy means it is not stationary. Another important characteristic of CPI is periodicity. That is, observations are made more than once a year, in this case twelve times per year. This shows that CPIs are monthly realizations of onward (monotone increasing) right-continuous random variable in positive real line.\nThe data I will be using is obtained from KNBS and runs from March 1962 to September 2020.\n\n\n2) Loading the Required Packages\nThe following packages are required to perform this modelling; -\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(ggTimeSeries)\nlibrary(data.table)\nlibrary(here)\n\n\n\n3) Loading Data\n\n# import data\nCPI_Data &lt;- read_excel(here(\"./Data/Historical CPI series 2020.xlsx\"))\n\n# view data\nhead(CPI_Data, 10)\n\n# A tibble: 10 × 3\n   `MONTHLY CPI FROM 1962` ...2  ...3               \n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt;              \n 1 &lt;NA&gt;                    &lt;NA&gt;  &lt;NA&gt;               \n 2 Year                    Month Index              \n 3 1962                    Mar   0.47503449374032269\n 4 &lt;NA&gt;                    Jun   0.48014239152247667\n 5 &lt;NA&gt;                    Sep   0.48525028930463066\n 6 &lt;NA&gt;                    Dec   0.49035818708678469\n 7 1963                    Mar   0.48525028930463066\n 8 &lt;NA&gt;                    Jun   0.49035818708678469\n 9 &lt;NA&gt;                    Sep   0.50057398265109265\n10 &lt;NA&gt;                    Dec   0.50568188043324669\n\n\nThe column names are in row 2, this can be corrected as;\n\n# assign headers \ncolnames(CPI_Data) &lt;- CPI_Data[2, ]\n\n# drop records when Month is blank or has the string \"Month\"\nCPI_Data_clean &lt;- CPI_Data %&gt;% \n  filter(!is.na(Month) & Month != \"Month\")\n\n# fill in blank years\nCPI_Data_clean &lt;- CPI_Data_clean %&gt;% \n  fill(Year, .direction = \"down\") %&gt;% \n  mutate(Index = as.numeric(Index),\n         Year = as.numeric(Year))\n\n# view data\nhead(CPI_Data_clean, 10)\n\n# A tibble: 10 × 3\n    Year Month Index\n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1  1962 Mar   0.475\n 2  1962 Jun   0.480\n 3  1962 Sep   0.485\n 4  1962 Dec   0.490\n 5  1963 Mar   0.485\n 6  1963 Jun   0.490\n 7  1963 Sep   0.501\n 8  1963 Dec   0.506\n 9  1964 Mar   0.501\n10  1964 Jun   0.501\n\n\n\n\n4) Declaring CPI as Time Series\n\nCPI_Data_ts &lt;- CPI_Data_clean %&gt;% \n  select(Year, Month, Index) %&gt;% \n  ts(start = c(1962, 3), \n        end = c(2020,9), \n        frequency = 1) \n\nclass(CPI_Data_ts)   # Check the class, technically this is a univariate ts\n\n[1] \"mts\"    \"ts\"     \"matrix\" \"array\" \n\n\n\n\n5) Visualisation of Trend\n\nCPI_Data_clean %&gt;% \n  # filter(Year &lt; 1984) %&gt;% \n  # mutate(Index = round(as.numeric(Index))) %&gt;% \n  ggplot(aes(x = Year, y = Index, colour = \"Data\")) + \n  geom_line(size = 0.5) +\n  geom_smooth(aes(color = \"Trend\"),\n              method = \"gam\", \n              size = 0.5,\n              lty = 2) +\n  scale_colour_manual(values = c(\"dodgerblue1\", \"maroon\")) +\n  scale_x_continuous(breaks = seq(1960, 2020, 5)) +\n  scale_y_continuous(breaks = seq(0, 110, 15)) +\n\n  labs(x = \"Year\",\n       y = \"CPI\",\n       title = \"CPI & It's Trend, 1962 to 2020\",\n       subtitle = \"The Trend is Approximated using GAM\",\n       caption = \"Figure 1\") +\n  \n  # custom theme\n      theme(plot.title = element_text(face = \"bold\",\n                                      hjust = 0.5,\n                                      size = 13.5,\n                                      family = \"serif\",\n                                      color = \"black\"),\n            plot.subtitle = element_text(face = \"italic\",\n                                      hjust = 0.5,\n                                      size = 9.5,\n                                      family = \"serif\",\n                                      color = \"black\"),\n            axis.title = element_text(face = \"bold\",\n                                      size = 11.5,\n                                      family = \"serif\",\n                                      color = \"black\"),\n            axis.text = element_text(face = \"plain\",\n                                     size = 10,\n                                     family = \"serif\",\n                                     color = \"black\"),\n            strip.text.x = element_text(face = \"bold\",\n                                        size = 13.5,\n                                        family = \"serif\",\n                                        color = \"black\"),\n            axis.text.x = element_text(angle = 0, \n                                       hjust = 1),\n            \n            legend.position = \"top\",\n            legend.title = element_blank(), \n            legend.text = element_text(face = \"plain\",\n                                       size = 10,\n                                       family = \"serif\",\n                                       color = \"black\"),\n            legend.key = element_rect(colour = NA, \n                                      fill = NA),\n            \n            legend.box = \"horizontal\",\n            legend.key.size = unit(0.5, 'cm'),\n            legend.spacing.x = unit(0.3, 'cm'),\n            legend.background = element_blank(),\n            \n            plot.background = element_rect(fill = \"white\",\n                                           color = \"black\", \n                                           linewidth = 1),\n            panel.grid = element_blank(),\n            panel.grid.minor.x = element_blank(),\n            panel.grid.major.x = element_blank(),\n            axis.line = element_line(color = \"black\"),\n            axis.ticks = element_line(color = \"black\"),\n            panel.background = element_blank()) +\n  guides(color = guide_legend(override.aes = list(fill = NA)))\n\n\n\n\n\n\n\n\nFigure 1 above shows upward rise in CPI over the years, the upwardness is indicative of trend, and therefore non-stationarity. This further implies that the CPI is constituted by multiplicative components; trend, seasonality, and random effects. Since it is multiplicative, applying log-transformation makes it easy to extract the random component by differencing, which also stabilizes variance and reduces seasonality. Even though the plot is ragged, seasonality is not immediately evident.\nTo extract seasonality, Seasonal Trend Loess decomposer (tslm()) is invoked. The decomposer, which works better than decompose(), is used for periodic time series, the periodicity (seasonal window) here is 4 months.\n\n\n6) Decomposition of CPI\n\nCPI_Data &lt;- ts(CPI_Data_clean$Index, start = c(1962, 3), end = c(2020, 9), frequency = 4)\n\ndecompose_df &lt;- tslm(CPI_Data ~ trend + fourier(CPI_Data, 2))\n\ntrend &lt;- coef(decompose_df)[1] + coef(decompose_df)['trend']*seq_along(CPI_Data)\n\ncomponents &lt;- cbind(\n  data = CPI_Data,\n  trend = trend,\n  season = CPI_Data - trend - residuals(decompose_df),\n  remainder = residuals(decompose_df))\n\nautoplot(components, facet = TRUE)\n\n\n\n\n\n\n\n\nThe plot above shows the individual components of CPI;\n\nTrend,\nSeasonality, and\nRandom error.\n\nHere, seasonal fluctuations are more evident. The trend has been smoothed, i.e. it is devoid of seasonality and random error. To explore seasonality:-\n\nadjust_df &lt;- CPI_Data - components[, 'season']\n\nautoplot(CPI_Data, series = \"Data\") +\n  autolayer(adjust_df, series = \"Seasonally adjusted\")\n\n\n\n\n\n\n\n\nIt is evident that the rate of consumption exponentially rose in the early ’80s to early ’90s. The sinusoidal aspect of seasonality is revealed by the plot below. Consumption goes down in April by a seasonal effect of 0.25 and goes up around July by an effect of 0.65 every year.\n\nplot(window(components[,3],\n            start = c(2016,1),\n            end = c(2018,1)),\n     main = \"Seasonlity of CPI, 2016 to 2018\",\n     xlab = \"Months\",\n     ylab = \"Effects\",\n     col = \"maroon\",\n     lwd = 2.5)\n\nabline(h = seq(-.7,.7,.1),\n       v = seq(2016, 2018, 1/12),\n       col = \"grey\",\n       lty = 2)\n\n\n\n\n\n\n\n\n\n\n7) Checking Normality Pictorially\n\ni) Stem and Leaf Plot\n\nstem(CPI_Data, scale = 1, width = 105)\n\n\n  The decimal point is at the |\n\n   0 | 5555555555555555555556666666666666666666677777788899\n   1 | 00001111223344445556678889\n   2 | 00334566899999\n   3 | 0011111222333444444567777777778899\n   4 | 000000111233444556777799\n   5 | 001122223466788899\n   6 | 02455788\n   7 | 0012344466\n   8 | 347\n   9 | 4555679\n  10 | 08\n  11 | 19\n  12 | 3\n  13 | 35\n  14 | 08\n  15 | 014\n  16 | 244777899\n  17 | 00111112234445678\n  18 | 1238\n  19 | 0\n\n\nThe above stem-and-leaf diagram shows the skewed distribution that CPI assumes. The skewness is to right.\n\n\nii) Histogram\nThe histogram in the figure below is skewed to the right, confirming what we found with the stem-and-leaf diagram. This skewness implies the data is not normally distributed. Furthermore, the skewness calls for log-transformation of the data.\n\nggplot(CPI_Data, aes(x = CPI_Data)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 3, \n                 colour = \"black\", \n                 fill = \"maroon\")+\n  geom_density(alpha = .2, \n               fill = \"dodgerblue1\")+\n  labs(title = \"Distribution of CPI\",\n       x = \"CPI\", \n       y = \"Density\")+\n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  theme_classic()\n\n\n\n\n\n\n\n\nHistogram of Log Transformed CPI\n\nLoggedCPI &lt;- CPI_Data %&gt;% log()  \n  \nggplot(LoggedCPI, aes(x = LoggedCPI)) +\n  geom_histogram(aes(y = ..density..),\n                 binwidth = 0.1, \n                 colour = \"black\", \n                 fill = \"maroon\")+\n  geom_density(alpha = .2, \n               fill = \"dodgerblue1\")+\n  labs(title = \"Logged Distribution of CPI\",\n       x = \"CPI\", \n       y = \"Density\")+\n  scale_y_continuous(expand = c(0,0))+\n  scale_x_continuous(expand = c(0,0))+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe histogram above with a superimposed kernel density plot for CPI log-transgeomed gives a vague hint of a bimodal distribution. This needs further transformation.\n\n\niii) Q-Q Plot\nThe plot below clearly illustrates how far the CPI is from the normal distribution. QQ Plots compare observed data to standardized theoretical normal data. The closeness of the plot to straight line indicates the closeness of the observed data to being normally distributed. For our case, the CPI deviates away at the tails to form an S-curve that defies normality.\n\nggplot(CPI_Data, aes(sample=CPI_Data))+\n  stat_qq(color = \"maroon\")+\n  labs(title = \"Q-Q Plot of CPI\",\n       x = \"Theoretical\", \n       y = \"Sample\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n8) Checking Normality Formally\n\ni) Jarque Bera Test\n\nCPI_Data %&gt;% jarque.bera.test() # From package \"tseries\"\n\n\n    Jarque Bera Test\n\ndata:  .\nX-squared = 57.318, df = 2, p-value = 3.577e-13\n\n\nThe hypothesis for this test is:-\n\n\\(H_0\\): CPI is normally distributed,\n\\(H_a\\): CPI is not normally distributed.\n\nHere, the p-value is smaller than 0.05 and thus we reject \\(H_0\\) and conclude at 95% level of confidence that CPI is indeed significantly skewed.\n\n\n\n9) Testing Stationarity Pictorially\n\na) Lagged Plots\nThe figure below shows plots of lags 1 to 20. The fact that there is a strong persistence of a straight line (autocorrelation) for lags above 20 excludes the dominance of an MA(q) in the series and gives a strong evidence for an AR(p). This persistence is due to the strong correlation (0.987) at lag one. This property is also called long memory. However, the series cannot be simply an AR(p) since the order of such an AR(p) would be too large a number and parsimony (idea of Occult’s Razor) would be violated. Therefore, the lagged plots give evidence of an ARMA(p,q) being the underlying best-case scenario.\n\nCPI_Data %&gt;% gglagplot(lag = 20, \n                       seasonal = TRUE) + \n    scale_color_viridis_d(option = \"viridis\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\nb) ACF and PACF\nThe figure below is a correlogram of ACF and ACF of CPI for the first 80 lags. The slowly decaying property in ACF further gives evidence of the series being non-stationary and hence an AR(p). Because of the trend, the observations will usually be on the same side of the series’ global mean. The terms \\((X(t+k)−μ(X))(X(t)−μ(X))\\) are positive, for this reason, the ACF is positive as well and is close to one, i.e. 0.987 at lag 1. The smooth, linear decline in ACF also is an indicator of the insignificance of the seasonal effect in the series as opposed to a wave-like decline for the seasonally strong series. Notice that ACF comes to zero at lag about 78, this is too persistent an effect of long term memory.\nThe PACF cuts off immediately after the associated lag. This is typical of non-stationary processes.\n\nCPI_Data %&gt;% ggAcf(lag.max = 100,\n                   plot = TRUE) +\n  labs(title = \"CPI ACF\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCPI_Data %&gt;% ggPacf(lag.max = 100,\n                    plot = TRUE) +\n  labs(title = \"CPI PACF\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n10) Testing Stationarity Formally\n\na) Augmented Dickey-Fuller Test (tests for unit root)\nThe Augmented Dickey-Fuller (ADF) Test has the following hypothesis:\n\n\\(H_0\\) Unit root exists (non-stationary)\n\\(H_1\\): No unit root Exists (stationary)\n\n\nadf.test(CPI_Data)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  CPI_Data\nDickey-Fuller = -0.52375, Lag order = 6, p-value = 0.9802\nalternative hypothesis: stationary\n\n\nWe fail to reject \\(H_0\\) at 95% confidence level because 0.9802 &gt; 0.05. The conclusion is that CPI for the period under study is not stationary. It follows from this conclusion that differencing is required to render the series stationary. As we have earlier deduced the underlying model to be ARMA(p,q), we now have a strong evidence for ARIMA (p,d,q) where d is the number of times required to difference to achieve stationarity.\n\n\nb) Ljung-Box Text (tests for white noise)\nTesting stationarity by checking whether the data is white noise, Ljung-Box (1978) test was made use of. The test is based on whether the sample autocorrelation is equal to zero:\n\n\\(H_0; ρ = 0\\) White noise\n\\(H_1; ρ ≠ 0\\) Not white noise.\n\nThis test follows a Chi-square distribution. If the p-value is less than 0.05 at lag \\(h\\) degrees of freedom, \\(H_0\\) is rejected and differencing is done.\nThe test statistic is given as; \\(Q(h) = n(n+2)∑^h _{k=1}\\frac{ρk}{(n−k)}\\)\n\nBox.test(CPI_Data, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  CPI_Data\nX-squared = 234.89, df = 1, p-value &lt; 2.2e-16\n\n\nClearly, the data is not white noise, we reject \\(H_0\\) and difference the data to achieve stationarity.\n\n\n\n11) Automatic Fitting of ARIMA(p,d,q)\nHaving established that CPI follows an ARIMA (p,d,q) model, we go ahead to find the best fitting model using AIC and BIC as the tools for choosing the parsimonious fit. The package forecast with appropriate dependencies installed, we are saved the trouble of transfoming the data to stabilize variance and seasonality before manually differencing.\n\nCPIfit &lt;- CPI_Data %&gt;% auto.arima(approximation = FALSE,\n                                  stepwise = FALSE,\n                                  trace = FALSE) # trace = TRUE will print all the possible models\nsummary(CPIfit)\n\nSeries: . \nARIMA(2,2,0)(2,0,1)[4] \n\nCoefficients:\n          ar1      ar2     sar1     sar2    sma1\n      -0.7461  -0.4174  -0.8993  -0.2587  0.8043\ns.e.   0.0610   0.0596   0.0983   0.0644  0.0831\n\nsigma^2 = 0.01867:  log likelihood = 137.05\nAIC=-262.1   AICc=-261.74   BIC=-241.29\n\nTraining set error measures:\n                      ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set 0.002888028 0.1346216 0.06989758 0.08317552 1.286139 0.2097828\n                    ACF1\nTraining set -0.03239534\n\n\nNotice that by setting approximation = FALSE and stepwise = FALSE a more advanced/rigorous search for a model in conducted. The best model has the smallest BIC and AIC. The non-seasonal order (2,2,0) means the series has a moving average of order 0, an autoregressive component of order 2, and the ARMA is differenced twice (d = 2) to achieve stationarity. The mathematical model is the product of MA and AR components.\nA pure AR(p) model is one where \\(Y_t\\) depends only on its own lags. That is, \\(Y_t\\) is a function of the ‘lags of \\(Y_t\\)’:\n\\[Y_t=α+β_1Y_{t−1}+β_2Y_{t−2}+...β_pY_{t−p}+ϵ_1\\]\nA pure MA(q) is is one that \\(Y_t\\) depends only on the lagged forecast errors and is given by:\n\\[Y_t=α+ϵ_t+ϕ_1ϵ_{t−1}+ϕ_2ϵ_{t−2}+...ϕ_{t−q}\\]\nOverall, ARIMA becomes:\n\\[Y_t=α+β_1Y_t−1+β_2Y_{t−2}+...+β_pY_{t−p}ϵ_t+ϕ_1ϵ_{t−1}+ϕ_2ϵ_{t−2}+...+ϕ_qϵ_{t−q}\\] Substituting the coefficients, this becomes: \\[Y_t=α-0.7461Y_{t−1}-0.4174Y_{t−2}-0.8993ϵ_{t−1}-0.2587ϵ_{t−2}+0.8043ϵ_{t−3}\\]\n\na) Check Residuals for White Noise\n\ncheckresiduals(CPIfit)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,2,0)(2,0,1)[4]\nQ* = 5.0548, df = 3, p-value = 0.1678\n\nModel df: 5.   Total lags used: 8\n\n\nThere are a few significant spikes in the ACF, and the model fails the Ljung-Box test. The model can still be used for forecasting, but the prediction intervals may not be accurate due to the correlated residuals. Sometimes it is just not possible to find a model that passes all of the tests.\n\n\nb) Forecasting\n\nCPIforecast &lt;- CPIfit %&gt;% \n  forecast(h = 125,  # Forecast 5 (each with 4 seasons) years ahead of Sept 2020\n           level = c(95, 99))\n\n# summary(CPIforecast) # Print forecasts\n\n\n\nc) Visualising Forecasts\n\nCPIforecast %&gt;% autoplot() + theme_bw()\n\n\n\n\n\n\n\n\nThe point forcasts with both 95% CI and 99% CI are contained in summary(CPIforecast) for 33 years (each with 4 seasons) ahead of 2020, that is upto Q2 2053.\nEnd\nNote: This post was first published by the author on RPubs in 2019."
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html",
    "href": "content/posts/Simulation_2024-09-12/index.html",
    "title": "Basics of Statistical Data Simulation",
    "section": "",
    "text": "Image source: OpenArt"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#sec-bands",
    "href": "content/posts/Simulation_2024-09-12/index.html#sec-bands",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.1 Bands",
    "text": "2.1 Bands\nThis is the outcome – a categorical 5-point ordinal variable where 1 represents the most needy and 5 represents the least needy2, thus x_{ij} where j = \\{1, 2, 3, ..., 5\\}. We will assume that the first 2 or 3 bands have a fairly higher chance of occurring compared to the last bands, i.e for the i^{th} student, p(x_{i1}) &gt; p(x_{i2}) &gt;... &gt;p(x_{i5}). We need a probability distribution function that draws 5 numbers between 1 to 5 for 10000 observations. A multinomial distribution will do just that. We simply need to specify n = 1 (create 1 vector), size = 10000 (number of observations), and prob = p(x_{ij}) (vector of probabilities) in the function rmultinom from the package {stats}. The function is the Application Programming Interface (API) to the RNG for multinomial distribution.\nIf we use a uniform probability of p(x_{ij}) = 1/5 we will end up with a uniform distribution, but we desire a distribution that is skewed to the left. Remember that the bands are a proxy for the socio-economic status of the household, and in Kenya – as in most other countries3 – the number of rich households is significantly lower than the number of poor households, therefore we need to expect more band 1 households than band 5 households.\n\n\nCode\n# generate 5 bands via multinomial distribution with equal probabilities\nset.seed(44)\nsample1 &lt;- rmultinom(n = 1,\n                     size = 10000,\n                     prob = rep(1/5, 5)) |&gt; \n  as.data.frame() |&gt;\n  dplyr::mutate(Counts = dplyr::row_number()) |&gt;\n  dplyr::rename(Summary = V1)\n\n# expand the Counts by Summary\nsample2 &lt;- rep(sample1$Counts, sample1$Summary)\n\n# visualize results\nbarplot(table(sample2), xlab  = \"Bands\", main = \"Histogram of Bands (Almost Uniform)\")\n\n\n\n\n\n\n\n\n\nWe could specify the 5 distinct probabilities p(x_{ij}) by trial and error, but the most effective way to get better probabilities is to think of each observation as a sum of 5 binomial trials (p(x) = (_x^n)p^xq^{n-x}) –not to be confused with multinomial – with the probability of success in each trial being p(x_{ij}) = 1/5. Each trial results in either a success (1) or a fail (0). Summing these outcomes, the lowest possible value will be 0 (all fails), and the highest will be 5 (all successes). Therefore, each student will be assigned a value between 0 and 5 (inclusive). As it turns out, 4s and 5s are significantly fewer than 0s and 1s, regardless of the seed value of the RNG. This is exactly what we want. Finally, declare x as an ordered factor because there is an intrinsic element of order or natural rank among the bands.\n\n\nCode\n# generate probabilities via 5 binomial trails with p = 1/5\n# 1 is added to remove band 0 and introduce band 5\nset.seed(44)\nprob &lt;- prop.table(table(rbinom(n = 10000, size = 5, prob = 1/5) + 1)) \n\n# generate 5 bands via multinomial distribution\nset.seed(44)\nsample1 &lt;- rmultinom(n = 1,\n                     size = 10000,\n                     prob = prob) |&gt; \n  as.data.frame() |&gt;\n  dplyr::mutate(Counts = dplyr::row_number()) |&gt;\n  dplyr::rename(Summary = V1)\n\n# expand the Counts by Summary\nsample2 &lt;- rep(sample1$Counts, sample1$Summary)\n\nbarplot(table(sample2),  xlab  = \"Bands\", main = \"Histogram of Bands (Appropriately Positively Skewed)\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#gross-family-income",
    "href": "content/posts/Simulation_2024-09-12/index.html#gross-family-income",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.2 Gross Family Income",
    "text": "2.2 Gross Family Income\nIncome, as with bands, is expected to be a positively skewed real-valued number (x ∈ ℝ^+) which can assume the negative binomial distribution with the mean of KES 20,0004. From this knowledge we can generate 10000 income values using rnbinom function from the package {stats} as shown in the code below;\n\n\nCode\nset.seed(44)\nhist(rnbinom(n = 10000, size = 5, mu = 20000),  xlab  = \"Income\", main = \"Histogram of Gross Family Income\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#geographical-location",
    "href": "content/posts/Simulation_2024-09-12/index.html#geographical-location",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.3 Geographical Location",
    "text": "2.3 Geographical Location\nWe will take all the 47 counties as the distinct geographical locations from which a student is equally likely to come – although realistically, certain counties have a relatively bigger share of student population, but for simplicity, we shall ignore this fact. With that out of the way, the appropriate probability distribution that ensures each student is allocated equal probability of being drawn from any of the 47 counties is the discrete uniform distribution. We can then simulate this variable using the function runif from the package {stats} as follows;\n\n\nCode\nset.seed(44)\nbarplot(table(ceiling(runif(n = 10000, min = 1,  max = 47))), xlab  = \"Geographical Location\", main = \"Discrete Uniform Distribution of Geographical Location\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#sec-pov_prob_ind",
    "href": "content/posts/Simulation_2024-09-12/index.html#sec-pov_prob_ind",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.4 Poverty Probability Index",
    "text": "2.4 Poverty Probability Index\nWe shall treat this as x \\sim N(\\mu, s) which is in the field x ∈ ℝ^+. This is the so-called normal distribution, and both \\mu and s are the mean and standard deviation apriori parameters. The probability density function itself is written as f(x) = \\frac{1}{\\sigma \\sqrt(2 \\pi)}e^{\\frac{-1}{2}(\\frac{x-\\mu}{\\sigma})^2}. According to the PPI tool, the mean index poverty for Kenya is approximately 0.3 and the standard deviation is approximately 0.2. With this apriori knowledge, we can plug the values into the rnorm function in the package {stats} as follows;\n\n\nCode\nset.seed(44)\nhist(abs(rnorm(n = 10000, mean = 0.3, sd = 0.2)), xlab  = \"Poverty Probability Index\", main = \"Normal Distribution of Poverty Probability Index\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#orphans",
    "href": "content/posts/Simulation_2024-09-12/index.html#orphans",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.5 Orphans",
    "text": "2.5 Orphans\nThis is a binary variable which indicates whether a student is an orphan or not, and therefore follows a binomial distribution with one trial (also called Bernoulli). We require the probability (rate/prevalence) of the status of being an orphan in Kenya for us to simulate the data for this variable. According to Lee et al. (2014), 22.2% of children aged 15 to 17 were orphans and vulnerable (OVC). Students joining university are mostly aged 17 to 20 years. We shall therefore use the rate of 0.222 to simulate this variable as shown below;\n\n\nCode\nset.seed(44)\nbarplot(table(rbinom(n = 10000, size = 1, prob = 0.222)), xlab  = \"Orphan Status\", main = \"Bargraph of the Distribution of Orphan Status\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#disability",
    "href": "content/posts/Simulation_2024-09-12/index.html#disability",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.6 Disability",
    "text": "2.6 Disability\nThis is yet another binary variable which indicates whether a student has disability or does not, and therefore follows a binomial distribution with one trial. We shall use the rate of 2.2%5. Plugging this value in the rbinom formula, we get;\n\n\nCode\nset.seed(44)\nbarplot(table(rbinom(n = 10000, size = 1, prob = 0.022)), xlab  = \"Disability Status\", main = \"Bargraph of the Distribution of Disability Status\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#number-of-dependents",
    "href": "content/posts/Simulation_2024-09-12/index.html#number-of-dependents",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.7 Number of Dependents",
    "text": "2.7 Number of Dependents\nThe variable represents counts, and therefore x ∈ ℕ^+. Both Poisson and negative binomial distributions could model the variable effectively, but this time round we shall focus on the former. The probability mass function for Poisson distribution is given as f(x) = \\frac{e^{–λ} λ^x}{x!} where λ is the parameter representing the average number of occurrences of a Poisson event per unit space or time. In this context, it is the number of dependents per household. We shall use 4 as the value for this parameter6 and plug into the function rpois in the package {stats} as follows;\n\n\nCode\nset.seed(44)\nbarplot(table(rpois(n = 10000, lambda = 4)), xlab  = \"Number of Dependents\", main = \"Bargraph of the Distribution of Number of Dependents\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#program-costs-kes",
    "href": "content/posts/Simulation_2024-09-12/index.html#program-costs-kes",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.8 Program Costs (KES)",
    "text": "2.8 Program Costs (KES)\nSimilar to Section 2.4, this variable is in the field x ∈ ℝ^+ where x \\sim N(\\mu, s) with parameters taken to be \\mu = 500,000 and s = 50,000. Feeding these values into the normal distribution RNG, we get;\n\n\nCode\nset.seed(44)\nhist(abs(rnorm(n = 10000, mean = 500000, sd = 50000)), xlab  = \"Program Costs\", main = \"Histogram of Distribution of Program Costs (KES)\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#gender",
    "href": "content/posts/Simulation_2024-09-12/index.html#gender",
    "title": "Basics of Statistical Data Simulation",
    "section": "2.9 Gender",
    "text": "2.9 Gender\nSimilar to Section 2.1,this variable follows a multinomial distribution with 3 outcomes. This time round, we know the three probabilities p(x_{ij}) from literature; male (49.0%), female (50.0%), and intersex (0.01%)7.\n\n\nCode\n# declare probabilities (values gotten from existing literature)\nprob &lt;- c(0.49, 0.50, 0.01)\n\n# generate 3 outcomes via multinomial distribution\nset.seed(44)\nsample1 &lt;- rmultinom(n = 1,\n                     size = 10000,\n                     prob = prob) |&gt; \n  as.data.frame() |&gt;\n  dplyr::mutate(Counts = dplyr::row_number()) |&gt;\n  dplyr::rename(Summary = V1)\n  \n# expand Counts by Summary\nsample2 &lt;- rep(sample1$Counts, sample1$Summary)\n\nbarplot(table(sample2), xlab  = \"Gender\", main = \"Bargraph of Gender\")"
  },
  {
    "objectID": "content/posts/Simulation_2024-09-12/index.html#footnotes",
    "href": "content/posts/Simulation_2024-09-12/index.html#footnotes",
    "title": "Basics of Statistical Data Simulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://kafu.ac.ke/images/2022/Academics/nfm/NEW_FUNDING_MODEL_-_6TH_AUGUST_2024.pdf↩︎\nhttps://www.universitiesfund.go.ke/wp-content/uploads/2024/03/Issue-December-2023.pdf↩︎\nhttps://www.researchgate.net/figure/US-Distribution-of-Income-Actual-vs-Lognormal-model_fig1_327971358↩︎\nhttps://www.businessdailyafrica.com/bd/economy/kenyans-average-income-of-sh20-123-hits-six-year-high–4043204↩︎\nhttps://devinit-prod-static.ams3.cdn.digitaloceanspaces.com/media/documents/Status-of-disability-in-Kenya__IF.pdf↩︎\nhttps://dhsprogram.com/pubs/pdf/SR277/SR277.pdf↩︎\nhttps://kenya.unfpa.org/en/topics/population-matters-0↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cornelius Tanui",
    "section": "",
    "text": "I am a skilled data professional with over 5 years of experience in public health, renewable energy, WASH, climate action, and clinical trials.\nHaving a B.Sc. in Applied Statistics with Computing, I have worked as a Data Analyst, Statistician, Statistical Trainer, Data Manager, and currently as an FSP Data Configurations Engineer."
  }
]